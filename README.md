# Lightweight Agentic Lead Intelligence RAG Service

# Lightweight Agentic RAG Service
Alright. Pivot. Calm. Strategic.

â¸»

ğŸš€ Agentic Lead Intelligence RAG

A lightweight, production-ready Agentic RAG (Retrieval-Augmented Generation) service that analyzes startup signals (hiring, funding, remote readiness) using semantic search + LLM reasoning.

Built with:
 â€¢ FastAPI
 â€¢ FAISS (vector search)
 â€¢ Sentence Transformers
 â€¢ Groq LLM (Llama 3.3 70B)
 â€¢ Docker-ready architecture

â¸»

ğŸ§  What This Project Does

This service:
 1. Embeds startup-related signals into a FAISS vector store
 2. Retrieves the most relevant context using semantic similarity
 3. Sends structured context to an LLM
 4. Returns structured JSON analysis

Example analysis output:
```bash
{
  "startup_name": "ExampleAI",
  "hiring_signal": true,
  "remote_possible": true,
  "funding_stage": "Seed",
  "reasoning": "Raised seed round and actively hiring Flutter developer.",
  "source_url": "https://example.com/post"
}
```

â¸»

ğŸ— Architecture
```bash
User Query
    â†“
Retriever (FAISS + Embeddings)
    â†“
Context Assembly
    â†“
Groq LLM (Structured JSON Output)
    â†“
FastAPI Response
```
Components:
 â€¢ embedding_service.py â†’ Generates sentence embeddings
 â€¢ vector_store.py â†’ FAISS index + persistence
 â€¢ retrieval_service.py â†’ Semantic retrieval logic
 â€¢ llm_service.py â†’ Groq structured JSON generation
 â€¢ main.py â†’ FastAPI endpoints

â¸»

âš™ï¸ Tech Stack
 â€¢ Python 3.10+
 â€¢ FastAPI
 â€¢ FAISS (CPU)
 â€¢ SentenceTransformers (all-MiniLM-L6-v2)
 â€¢ Groq LLM API
 â€¢ Docker

â¸»

ğŸš€ Getting Started

1ï¸âƒ£ Clone
```bash
git clone https://github.com/yourusername/agentic-lead-rag.git
cd agentic-lead-rag
```

â¸»

2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

â¸»

3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

â¸»

4ï¸âƒ£ Configure Environment

Create .env file:
```bash
GROQ_API_KEY=your_groq_api_key_here
```

â¸»

5ï¸âƒ£ Run the API
```bash
uvicorn app.main:app --reload
```
Open:

http://127.0.0.1:8000/docs

Use /analyze endpoint.

â¸»

ğŸ³ Docker Support

Build image:
```bash
docker build -t agentic-lead-rag .
```
Run container:
```bash
docker run -p 8000:8000 --env-file .env agentic-lead-rag
```

â¸»

ğŸ“¦ Features
 â€¢ Structured JSON enforcement from LLM
 â€¢ Async Groq integration
 â€¢ Semantic search retrieval
 â€¢ Source URL tracking
 â€¢ FAISS index persistence
 â€¢ Dockerized for portability

â¸»

ğŸ”¬ Example Query
```bash
"Startup hiring Flutter developer remotely after seed funding"
```
Returns structured intelligence analysis based on stored signals.

â¸»

ğŸ§­ Roadmap
 â€¢ Live Reddit ingestion
 â€¢ LinkedIn signal scraping
 â€¢ Scheduled background refresh
 â€¢ Frontend dashboard
 â€¢ Multi-source ingestion pipeline
 â€¢ Deployment (Render / Railway)
 â€¢ Usage-based monetization

â¸»

ğŸ¯ Why This Matters

This is not a chatbot.

It is a structured intelligence engine designed to extract startup signals for:
 â€¢ Freelancers
 â€¢ Recruiters
 â€¢ Founders
 â€¢ Investors

â¸»

ğŸ§± Author

Built by Me â€” AI-focused mobile + systems engineer exploring Agentic architectures and applied intelligence systems.

â¸»

ğŸ“„ License

MIT License

â¸»

Thatâ€™s clean. No hype. No cringe. Serious engineering tone.

â¸»


Dockerized FastAPI agent for RAG tasks.

## Run locally with Docker

```bash
docker build -t lead-rag-agent .
docker run -d -p 8000:8000 --env-file .env -v "${PWD}:/app" lead-rag-agent
```

Visit: http://localhost:8000/docs

